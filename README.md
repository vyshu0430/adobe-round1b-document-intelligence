
# Round 1B â€“ Persona-Driven Document Intelligence

This project is part of Adobe's "Connecting the Dots" Challenge â€“ Round 1B. It processes a collection of PDF documents and extracts the most relevant sections and sub-sections based on an inferred user persona and job-to-be-done.

## ğŸ§  Key Features

- Automatically infers persona and task from PDFs
- Extracts and ranks headings (H1, H2, H3)
- Ranks sub-sections by semantic relevance using transformer embeddings
- Outputs results in structured JSON format
- Fully Dockerized, offline, and CPU-compliant

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ input/                 # Folder containing input PDFs
â”œâ”€â”€ output/                # Output result.json
â”œâ”€â”€ all-MiniLM-L6-v2/      # Pre-downloaded model
â”œâ”€â”€ main_b.py              # Main script
â”œâ”€â”€ Dockerfile             # Docker setup
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ approach_explanation.md
â”œâ”€â”€ README.md
```

## âš™ï¸ Build & Run Instructions

### ğŸ”§ Build Docker Image
```
docker build --platform linux/amd64 -t round1b_solution:v1 .
```

### ğŸš€ Run the Container
```
docker run --rm -v $(pwd)/input:/app/input -v $(pwd)/output:/app/output --network none round1b_solution:v1
```

## ğŸ›  Requirements

- Python 3.10
- PyMuPDF
- sentence-transformers
- torch

## ğŸ“¦ Model Info

- Model: `all-MiniLM-L6-v2` (from sentence-transformers)
- Size: ~90MB
- Stored locally in: `all-MiniLM-L6-v2/` folder for offline usage

---

This solution is fully self-contained, platform-independent, and complies with all performance, offline, and CPU-only constraints defined in the challenge.
